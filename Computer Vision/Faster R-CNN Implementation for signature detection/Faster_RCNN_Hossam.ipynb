{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889d65a1",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2659cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.vgg import vgg16\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46302fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857a104",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = 'signature_data'\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_ROOT, 'train', 'images')\n",
    "TRAIN_LABELS_DIR = os.path.join(DATA_ROOT, 'train', 'labels')\n",
    "TEST_IMAGES_DIR = os.path.join(DATA_ROOT, 'test', 'images')\n",
    "TEST_LABELS_DIR = os.path.join(DATA_ROOT, 'test', 'labels')\n",
    "\n",
    "# Create custom dataset\n",
    "class SignatureDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Get original dimensions\n",
    "        orig_width, orig_height = image.size\n",
    "        \n",
    "        # Load bounding box from text file\n",
    "        # Assuming label file has same name as image but with .txt extension\n",
    "        label_name = os.path.splitext(img_name)[0] + '.txt'\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "        \n",
    "        boxes = []\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                # Reading x1, y1, x2, y2 coordinates\n",
    "                coords = list(map(float, f.read().strip().split()))\n",
    "                \n",
    "                # Ensure we have 4 coordinates\n",
    "                if len(coords) == 4:\n",
    "                    x1, y1, x2, y2 = coords\n",
    "                    # Convert to [x1, y1, x2, y2] format and normalize to [0, 1]\n",
    "                    # If coordinates are already normalized, skip normalization\n",
    "                    if max(coords) > 1:\n",
    "                        x1 /= orig_width\n",
    "                        x2 /= orig_width\n",
    "                        y1 /= orig_height\n",
    "                        y2 /= orig_height\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "                else:\n",
    "                    print(f\"Warning: Incorrect format in {label_path}\")\n",
    "                    # Add a dummy box to avoid errors\n",
    "                    boxes.append([0.0, 0.0, 1.0, 1.0])\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Label file not found: {label_path}\")\n",
    "            # Add a dummy box to avoid errors\n",
    "            boxes.append([0.0, 0.0, 1.0, 1.0])\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = torch.ones((len(boxes),), dtype=torch.int64)  # Class 1 for signature\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        target[\"area\"] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Denormalize boxes to pixel coordinates for model input\n",
    "        h, w = image.shape[-2:]\n",
    "        boxes_denorm = boxes.clone()\n",
    "        boxes_denorm[:, [0, 2]] *= w\n",
    "        boxes_denorm[:, [1, 3]] *= h\n",
    "        target[\"boxes\"] = boxes_denorm\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f123e303",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((800, 800)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SignatureDataset(TRAIN_IMAGES_DIR, TRAIN_LABELS_DIR, transform=data_transform)\n",
    "test_dataset = SignatureDataset(TEST_IMAGES_DIR, TEST_LABELS_DIR, transform=data_transform)\n",
    "\n",
    "# Collate function for handling batches of variable size\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Build Faster R-CNN model with VGG16 backbone\n",
    "def get_model(num_classes=2):  # Background + Signature\n",
    "    # Load VGG16 pretrained on ImageNet\n",
    "    backbone = vgg16(pretrained=True).features\n",
    "    \n",
    "    # Fix backbone parameters\n",
    "    for param in backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Unfreezing the last conv layers in VGG16\n",
    "    for layer in list(backbone)[-4:]:\n",
    "        if hasattr(layer, 'weight'):\n",
    "            layer.weight.requires_grad = True\n",
    "        if hasattr(layer, 'bias'):\n",
    "            layer.bias.requires_grad = True\n",
    "    \n",
    "    # VGG16 backbone returns a feature map with 512 channels\n",
    "    backbone_out_channels = 512\n",
    "    \n",
    "    # Define anchor generator\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32, 64, 128, 256, 512),),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),)\n",
    "    )\n",
    "    \n",
    "    # Define ROI pooler\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
    "        featmap_names=['0'],\n",
    "        output_size=7,\n",
    "        sampling_ratio=2\n",
    "    )\n",
    "    \n",
    "    # Create Faster R-CNN model\n",
    "    model = FasterRCNN(\n",
    "        backbone=backbone,\n",
    "        num_classes=num_classes,\n",
    "        rpn_anchor_generator=anchor_generator,\n",
    "        box_roi_pool=roi_pooler,\n",
    "        min_size=800,\n",
    "        max_size=800\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "model = get_model()\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloaders, optimizer, lr_scheduler, num_epochs=10):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training phase\n",
    "        model.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Iterate over data\n",
    "        for images, targets in tqdm(dataloaders):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Backward pass\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += losses.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloaders)\n",
    "        \n",
    "        print(f'Training Loss: {epoch_loss:.4f}')\n",
    "        \n",
    "        # Deep copy the model if it's the best so far\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # Adjust the learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best Loss: {best_loss:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "model = train_model(model, train_loader, optimizer, lr_scheduler, num_epochs=num_epochs)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), os.path.join(DATA_ROOT, 'faster_rcnn_signature_model.pth'))\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Parameters for calculating metrics\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        \n",
    "        # IoU threshold for considering a detection correct\n",
    "        iou_threshold = 0.5\n",
    "        \n",
    "        for images, targets in tqdm(data_loader):\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Make predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Process each image in the batch\n",
    "            for i, (output, target) in enumerate(zip(outputs, targets)):\n",
    "                pred_boxes = output['boxes'].cpu()\n",
    "                pred_scores = output['scores'].cpu()\n",
    "                target_boxes = target['boxes'].cpu()\n",
    "                \n",
    "                # Filter predictions by confidence score\n",
    "                conf_threshold = 0.5\n",
    "                keep = pred_scores > conf_threshold\n",
    "                pred_boxes = pred_boxes[keep]\n",
    "                \n",
    "                # If no predictions or no target boxes, update counts\n",
    "                if len(pred_boxes) == 0 and len(target_boxes) > 0:\n",
    "                    false_negatives += len(target_boxes)\n",
    "                    continue\n",
    "                elif len(target_boxes) == 0 and len(pred_boxes) > 0:\n",
    "                    false_positives += len(pred_boxes)\n",
    "                    continue\n",
    "                elif len(pred_boxes) == 0 and len(target_boxes) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate IoU for each prediction with each target\n",
    "                ious = box_iou(pred_boxes, target_boxes)\n",
    "                max_ious, max_indices = ious.max(dim=1)\n",
    "                \n",
    "                # True positives: predictions with IoU > threshold\n",
    "                tp = (max_ious > iou_threshold).sum().item()\n",
    "                true_positives += tp\n",
    "                \n",
    "                # False positives: predictions with IoU <= threshold\n",
    "                fp = (max_ious <= iou_threshold).sum().item()\n",
    "                false_positives += fp\n",
    "                \n",
    "                # False negatives: targets without matching predictions\n",
    "                # A target is matched if any prediction has IoU > threshold with it\n",
    "                matched_targets = set()\n",
    "                for j, iou in enumerate(max_ious):\n",
    "                    if iou > iou_threshold:\n",
    "                        matched_targets.add(max_indices[j].item())\n",
    "                \n",
    "                fn = len(target_boxes) - len(matched_targets)\n",
    "                false_negatives += fn\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        return precision, recall, f1\n",
    "\n",
    "# Calculate IoU (Intersection over Union)\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = (boxes1[:, 2] - boxes1[:, 0]) * (boxes1[:, 3] - boxes1[:, 1])\n",
    "    area2 = (boxes2[:, 2] - boxes2[:, 0]) * (boxes2[:, 3] - boxes2[:, 1])\n",
    "    \n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "    \n",
    "    wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "    \n",
    "    union = area1[:, None] + area2 - inter\n",
    "    \n",
    "    iou = inter / union\n",
    "    return iou\n",
    "\n",
    "# Test the model\n",
    "print(\"Evaluating model on test data...\")\n",
    "precision, recall, f1 = evaluate_model(model, test_loader)\n",
    "\n",
    "# Visualization function\n",
    "def visualize_results(model, data_loader, num_images=5):\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axs = plt.subplots(num_images, 2, figsize=(15, 5*num_images))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "            if i >= num_images:\n",
    "                break\n",
    "                \n",
    "            img = images[0].permute(1, 2, 0).cpu().numpy()\n",
    "            # Denormalize image\n",
    "            img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            # Get predictions\n",
    "            images = list(img.to(device) for img in images)\n",
    "            predictions = model(images)\n",
    "            \n",
    "            # Ground truth\n",
    "            axs[i, 0].imshow(img)\n",
    "            axs[i, 0].set_title(\"Ground Truth\")\n",
    "            boxes = targets[0]['boxes'].cpu().numpy()\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box\n",
    "                axs[i, 0].add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                               fill=False, edgecolor='red', linewidth=2))\n",
    "            \n",
    "            # Predictions\n",
    "            axs[i, 1].imshow(img)\n",
    "            axs[i, 1].set_title(\"Predictions\")\n",
    "            pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "            pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "            \n",
    "            # Filter by confidence\n",
    "            keep = pred_scores > 0.5\n",
    "            pred_boxes = pred_boxes[keep]\n",
    "            pred_scores = pred_scores[keep]\n",
    "            \n",
    "            for j, box in enumerate(pred_boxes):\n",
    "                x1, y1, x2, y2 = box\n",
    "                axs[i, 1].add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                               fill=False, edgecolor='blue', linewidth=2))\n",
    "                axs[i, 1].text(x1, y1, f\"Score: {pred_scores[j]:.2f}\", \n",
    "                             bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(DATA_ROOT, 'results_visualization.png'))\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some results\n",
    "print(\"Visualizing detection results...\")\n",
    "visualize_results(model, test_loader, num_images=5)\n",
    "\n",
    "# Inference function for new images\n",
    "def predict_signature(model, image_path):\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((800, 800)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)\n",
    "    \n",
    "    # Process prediction\n",
    "    pred_boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    pred_scores = prediction[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    # Filter by confidence\n",
    "    confidence_threshold = 0.5\n",
    "    keep = pred_scores > confidence_threshold\n",
    "    pred_boxes = pred_boxes[keep]\n",
    "    pred_scores = pred_scores[keep]\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    img = np.array(image)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    for i, box in enumerate(pred_boxes):\n",
    "        x1, y1, x2, y2 = box\n",
    "        plt.gca().add_patch(plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                         fill=False, edgecolor='blue', linewidth=2))\n",
    "        plt.gca().text(x1, y1, f\"Score: {pred_scores[i]:.2f}\", \n",
    "                     bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_boxes, pred_scores\n",
    "\n",
    "# Example usage of inference function\n",
    "# predict_signature(model, \"path/to/new/document.jpg\")\n",
    "\n",
    "print(\"Faster R-CNN model for signature detection completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
